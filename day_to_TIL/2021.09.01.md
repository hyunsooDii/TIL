## 2021.09.01 TIL

### 파이썬 패키지

- ##### Numpy

  - Numpy는 수치 데이터를 다루기 위한 파이썬 패키지
  - 행렬이나 다차원 배열로 벡터 및 행렬을 쉽게 할 수 있으며 효율적인 수치 계산 기능을 제공

- ##### Pandas

  - Pandas는 dataframe을 다루기 위한 패키지
  - 데이터 기초분석 및 EDA를 위한 기능을 제공

- ##### Matplotlib

  - Matplolib은 그래프 패키지임
  - 차트나 플롯으로 시각화하는 기능을 제공



### Numpy

##### 배열생성

`````python
import numpy as np

a = np.array([1, 2, 3, 4, 5])
print(a)
print(type(a))
print(a.ndim) # 배열의 차원 출력
print(a.shape) # 배열의 모양(크기) 출력

# 리스트를 가지고 2차원 배열을 생성
# array() 안에 하나의 리스트만 들어가므로 리스트안에 리스트를 포함하여 생성
b = np.array([[1, 2, 3], [ 4, 5, 6]]) 
print(b)  
print(b.ndim) # 배열의 차원 출력
print(b.shape) # 배열의 모양 출력

# 지정해준 범위에 대해 배열을 생성
# numpy.arange(start, stop, step, dtype)
?np.arange
# a = np.arange(n) # 0, ..., n-1까지 범위의 지정.
# a = np.arange(i, j, k) # i부터 j-1까지 k씩 증가하는 배열.
# numpy의 arange는 Python의 range와 비슷하지만 배열(array)을 반환한다.

y = np.arrange(10)
print(y)

#1부터 19까지 +2씩 적용되는 범위
a = np.arange(1, 20, 2) 
print(a)
`````



##### 배열 초기화

- zeros()는 해당 배열에 모두 0을 삽입
- ones()는 모두 1을 삽입
- zeros_like(), ones_like() 크기를 복사한 배열 생성
- full()은 배열에 사용자가 지정한 값을 넣는데 사용
- eye()는 대각선으로는 1이고 나머지는 0인 2차원 배열을 생성
- random()은 임의의 난수 값으로 채워진 배열을 생성

```python
# 모든값이 0인 2 x 4 배열 생성
a = np.zeros((5,7)) 
print(a)

# 모든값이 1인 2 x 4 배열 생성
a = np.ones((2,4)) 
print(a)

b = np.zeros_like(a)
print(b)

c = np.ones_like(b)
print(c)

# 모든 값이 특정 상수(4)인 배열 생성
a = np.full((2, 3),4) 
print(a)

# 대각선으로는 1이고 나머지는 0인 3 * 3 배열을 생성
a = np.eye(3) 
print(a)

# 임의의 값으로 채워진 배열 생성
a = np.random.random((2, 4)) 
print(a)
```



##### 배열 연산

- 배열간 연산은 +, -, *, /의 연산자를 사용할 수 있으며, add(), subtract(), multiply(), divide() 함수를 사용할 수도 있다.

`````python
x = np.array([1,2,3])
y = np.array([4,5,6])
b = x + y # 각 배열 요소에 대해서 더함
print(b)

# add() 함수의 결과도 동일함
b = np.add(x, y)
print(b)
`````



##### 배열 인덱싱 & 슬라이싱

```python
#1차원 array
a = np.array([0,1,2,3,4])

a[2]

a[-1]

#다차원 array
a = np.array([[0,1,2], [3,4,5]])
a

#첫번째 행의 첫번째 열
a[0,0]

#마지막 행의 마지막 열 
a[-1, -1]

#마지막 행의 마지막에서 두번째 열 
a[-1, -2]

a = np.array([[0,1,2,3], [4,5,6,7]])
a

#첫번째 행 전체 
a[0, :]

#두번째 열 전체
a[:, 1]

#두번째 행의 두번째 열부터 끝까지

a[1, 1:]
```



##### 배열 크기 변형

```python
a = np.arange(12)
print(a)

b = a.reshape(3,4)
print(b)

c = a.reshape(3, -1)
print(c)

# 배열을 1차원으로 만들기 
d = c.flatten()
print(d)
```



##### 기술통계

- 데이터의 개수(count)
- 평균(mean)
- 분산(variance) : 편차(=관측값-평균)의 제곱의 평균
- 표준 편차(standard deviation): 분산에 루트를 씌운 값 (자료의 흩어짐 정도)
- 최댓값(maximum)
- 최솟값(minimum)
- 중앙값(median)
- 사분위수(quartile)

```python
x = np.array([18,   5,  10,  23,  19,  -8,  10,   0,   0,   5,   2,  15,   8,
              2,   5,   4,  15,  -1,   4,  -7, -24,   7,   9,  -6,  23, -13])
#데이터의 갯수 
len(x)

#평균
np.mean(x)

#분산
np.var(x)

#표준편차
np.std(x)

print(np.max(x))
print(np.min(x))
print(np.median(x)) #중앙값

print(np.percentile(x, 0)) #최소값
print(np.percentile(x, 25)) #1사분위수
print(np.percentile(x, 50)) #2사분위수 (중앙값)
print(np.percentile(x, 75)) #3사분위수
print(np.percentile(x, 100)) #최댓값

```



### Pandas 

##### Series

- 1차원 배열의 형태를 가진 자료구조

```python
import pandas as pd
s = pd.Series([1,3,5,np.nan,6,8])

print(s)
```

##### 데이터 프레임

- 데이터프레임은 2차원 테이블의 형태를 가지며 행방향 인덱스(index)와 열방향 칼럼(column)이 존재하는 행과 열을 가지는 자료구조 (데이터 객체)

`````python
import pandas as pd

values = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
index = ['첫째행', '둘째행', '셋째행']
columns = ['컬럼1', '칼럼2', '칼럼3']

df = pd.DataFrame(values, index=index, columns=columns)
print(df)
`````



##### 데이터 프레임의 생성

- 데이터프레임은 리스트(List), 딕셔너리(dict), Numpy의 ndarrays, 또 다른 데이터프레임으로 생성할 수 있음

`````python
# 리스트로 생성하기
data = [
    ['100', '이순신', 9.7], 
    ['101', '강감찬', 8.9], 
    ['102', '을지문덕', 9.3], 
    ['103', '김유신', 6.1], 
]
df = pd.DataFrame(data)
print(df)

# 생성된 데이터프레임에 열(columns)을 지정 
df = pd.DataFrame(data, columns=['학번', '이름', '점수'])
print(df)

# 딕셔너리로 생성하기
data = { '학번' : ['100', '101', '102', '103'],
'이름' : [ '이순신', '강감찬', '을지문덕', '김유신'],
         '점수': [9.7, 8.9, 9.3, 6.1,]}

df = pd.DataFrame(data)
print(df)
`````



##### 데이터 프레임의 접근

- df.head(n) - 앞 부분을 n개만 보기
- df.tail(n) - 뒷 부분을 n개만 보기
- df['열이름'] - 해당되는 열을 확인

`````python
# print(df.head(3)) # 앞 부분을 3개만 보기
# print(df.tail(3)) # 뒷 부분을 3개만 보기
print(df['이름']) # '이름'에 해당되는 열을 보기

#dataframe 기술통계
print(df.describe())

#행열 변경 
print(df.T)

#정렬 
print(df)
print('-------')
print(df.sort_index(axis=0, ascending=False)) #index를 기준으로 내림차순 정렬
print('-------')
print(df.sort_index(axis=1, ascending=True)) #컬럼을 기준으로 오름차순 정렬

print(df['점수'] > 8) # 점수가 8 이상이면 True 아니면 False
print(df[df['점수'] > 9]) # 점수가 9 이상인 데이터만 출력

#앞서 만든 학번-점수 데이터 프레임을 학점이 높은 순으로 sorting 하시오.
print(df.sort_values(by='점수', ascending=False))
`````



### Matplotlib

`````python
import matplotlib.pyplot as plt
`````

##### 라인 플롯

`````python
plt.title('test') # 그래프 제목
plt.plot([1,2,3,4],[2,4,8,6]) # X값과 Y값을 정의
plt.show()
`````

##### 레이블 삽입

`````python
plt.title('test')
plt.plot([1,2,3,4],[2,4,8,6])
plt.xlabel('hours') # x축 레이블
plt.ylabel('score') # y축 레이블
plt.show()
`````

##### 범례 삽입

`````python
plt.title('test')
plt.plot([1,2,3,4],[2,4,8,6]) # 라인1
plt.plot([1.5,2.5,3.5,4.5],[3,5,8,10]) # 라인2
plt.xlabel('hours')
plt.ylabel('score')
plt.legend(['A', 'B']) # 라인1, 라인2의 범례 삽입
plt.show()
`````

##### 카테고리별 밸류

`````python
names = ['group_a', 'group_b', 'group_c']
values = [1, 10, 100]

plt.figure(figsize=(9,3)) #figure 객체 생성하기 

plt.subplot(131)           # 1 x 3 행렬의 첫번째 그림
plt.bar(names, values)

plt.subplot(132)
plt.scatter(names, values)

plt.subplot(133)
plt.plot(names, values)

plt.suptitle('Categorical plotting')
plt.show()
`````



### KNN 분류

##### 도미 데이터 준비하기

`````python
bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]
bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0]
`````

`````python
import matplotlib.pyplot as plt

plt.scatter(bream_length, bream_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
`````



##### 빙어 데이터 준비하기

`````python
smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
`````

`````python
plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
`````

##### 데이터 준비하기

`````python
length = bream_length+smelt_length
weight = bream_weight+smelt_weight
fish_data = [[l, w] for l, w in zip(length, weight)] #list comprehension

print(fish_data)

fish_target = [1]*35 + [0]*14 # 1 = bream(도미), 0 = smelt(빙어)
print(fish_target)
`````

##### 분류 알고리즘 실행

`````python
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier()
kn.fit(fish_data, fish_target) #학습

kn.score(fish_data, fish_target) #학습 데이터 스코어 

plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
plt.scatter(30, 600, marker='^')
plt.scatter(20, 400, marker='*')
plt.scatter(10, 100, marker='p')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

kn.predict([[30, 600], [20, 400], [10, 100]])
print(kn._fit_X)
## 참고 데이터를 49개로 한 모델 
kn49 = KNeighborsClassifier(n_neighbors=49)
kn49.fit(fish_data, fish_target)
kn49.score(fish_data, fish_target)
`````

### 데이터 분할 - 훈련, 테스트

##### numpy로 셔플링된 인덱스 만들기

`````python
import numpy as np
input_arr = np.array(fish_data)
target_arr = np.array(fish_target)

#index shuffle 
np.random.seed(42)
index = np.arange(49)
print(index)
np.random.shuffle(index)
print(index)
`````

##### 훈련데이터와 테스트 데이터로 분리

`````python
#학습 데이터(35개) 만들기 
train_input = input_arr[index[:35]]
train_target = target_arr[index[:35]]

print(input_arr[13], train_input[0]) #만들어진 첫번째 인덱스의 값 13과 훈련용 데이터의 0번째 인덱스의 값 비교 

#테스트 데이터(14개) 만들기
test_input = input_arr[index[35:]]
test_target = target_arr[index[35:]]

#산점도로 학습 데이터와 테스트 데이터 분포 확인하기 

import matplotlib.pyplot as plt

plt.scatter(train_input[:,0], train_input[:,1]) # x=length, y=weight
plt.scatter(test_input[:,0], test_input[:,1])

plt.xlabel('length')
plt.ylabel('weight')
plt.show()
`````



##### KNN 모델 생성 - 데이터 분할 후

`````python
kn = KNeighborsClassifier()
kn.fit(train_input, train_target)

kn.score(test_input, test_target)

kn.predict(test_input)
`````



##### 표준화된 데이터 학습하기(by데이터 전처리)

`````python
fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8, 
                10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]

# 리스트를 옆으로 붙이기 
np.column_stack(([1,2,3], [4,5,6]))

# Length와 Weight를 옆으로 붙이기
fish_data = np.column_stack((fish_length, fish_weight,loss
`````



\# stratify를 사용하여, 클래스 비율에 맞게 데이터 분할하기 

`````python(
train_input, test_input, train_target, test_target = train_test_split(fish_data, fish_target, stratify=fish_target, random_state=42)
`````

##### 수상한 도미 한마리

`````python
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier()
kn.fit(train_input, train_target)
kn.score(test_input, test_target)
`````

`````python
import matplotlib.pyplot as plt

plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^')
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

print(train_input[indexes])
print(train_target[indexes])
print(distances)
`````

##### 데이터 기준을 맞춰라

- Weight는 단위가 크고, Length는 단위가 작음. 도미와 빙어를 선택할 때 Weight가 상대적으로 큰 영향을 미치게 됨
- 오분류 발생 가능성 높아짐
- 두 특성의 스케일이 다르기 때문에 발생하는 문제임. 스케일을 맞추어 분류 정확도를 높일 수 있음

`````python
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^')
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')
plt.xlim((0, 1000)) # X의 기준을 Y의 기준인 0~1000과 맞추어 그래프 그리기. 
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

#평균과 표준편차 구하기 
#표준편차는 분산의 제곱근으로 데이터가 분산된 정도를 나타냄. 분산은 데이터에서 평균을 뺀 값을 모두 제곱한 다음 평균한 것. 
mean = np.mean(train_input, axis=0) #axis = 0 : 세로축, axis=1 : 가로축 
std = np.std(train_input, axis=0)

print(mean, std)

#표준 스코어 구하기 = 실제 데이터에서 평균을 빼고 표준편차로 나누기
#표준점수는 각 데이터가 원점에서 몇 표준편차만큼 떨어져 있는지를 나타내는 값. 
train_scaled = (train_input - mean) / std
print(train_scaled)
`````

##### 전처리 데이터로 모델 분류하기

`````python
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(25, 150, marker='^') #표준 점수로 변환 전
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

# 25, 150 데이터를 평균과 표준편차를 이용해서 표준점수로 변환하기 
new = ([25, 150] - mean) / std

plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()

#표준점수로 변환한 데이터를 훈련시키기 
kn.fit(train_scaled, train_target)

#테스트 데이터도 표준점수로 변환하기 
test_scaled = (test_input - mean) / std

print(kn.predict([new]))

# 길이 25, 무게 150g인 생선의 최근접 이웃 데이터 그리기 
distances, indexes = kn.kneighbors([new])

plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker='^')
plt.scatter(train_scaled[indexes,0], train_scaled[indexes,1], marker='D')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
`````

### 로지스틱 회귀

- 생선을 분류하는 문제
- 길이(Length), 높이(Height), 대각선(Diagonal), 두께(Width)를 특성(Features)으로 이용
- 분류 타겟은 Bream(도미), Roach(잉어), Whitefish(송어), Parkki, Perch(농어), pike(강꼬치), smelt(빙어)

##### 데이터 준비하기

`````python
#판다스를 이용하여 데이터 로드 
import pandas as pd
fish = pd.read_csv('https://bit.ly/fish_csv_data')
fish.head() #fish.head(n=3)

print(fish.shape)
print('--')
print(pd.unique(fish['Species'])) #고유한 값을 추출

# input 데이터 생성 
fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()

print(fish_input[:5])

#학습및 테스트용 데이터 분리
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)
print(train_input.shape, test_input.shape, train_target.shape)
print(train_input[:5])

#데이터 척도 표준화 - 값의 규모가 큰 혹은 작은 독립변수가 종속변수에 큰 영향을 주는 것을 방지하기 위해 척도를 표준화
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

print(train_scaled[:5])

`````

##### K 최근접 이웃의 다중분류

`````python
# 근접한 이웃의 수에 따라서 분류하는 확률적 방법 
from sklearn.neighbors import KNeighborsClassifier
kn= KNeighborsClassifier(n_neighbors=3)

kn.fit(train_scaled, train_target) # 학습

print(kn.score(train_scaled, train_target))
print(kn.score(test_scaled, test_target))

print(kn.classes_) #타겟값

#테스트 데이터로 예측한 값 
print(kn.predict(test_scaled[:5])) 

#확률 출력 
import numpy as np 
proba = kn.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=4))

`````

##### 로지스틱 회귀로 이진 분류 수행하기

- bream(도미), smelt(빙어)로 이진 분류 수행하기
- 도미와 빙어 데이터만 별도로 구분하여 학습

`````python
#도미와 빙어 데이터 구분하기 
bream_smelt_indexes = (train_target=='Bream')  | (train_target =="Smelt") #Bream, Smelt 데이터의 불린 인덱스 추출하기 

print(bream_smelt_indexes[:5])

train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]

print(train_bream_smelt[:5])
print(target_bream_smelt[:5])

#로지스틱 회귀 학습 

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_bream_smelt, target_bream_smelt)
print(lr.score(train_bream_smelt, target_bream_smelt))

print(lr.predict(train_bream_smelt[:5])) 
print(lr.predict_proba(train_bream_smelt[:5])) #확률 값 출력 

#로지스틱 회귀 계수 확인
print(lr.classes_)
print(lr.coef_, lr.intercept_)

# Z 값 계산하기 
decisions = lr.decision_function(train_bream_smelt[:5])
print(decisions)

#시그모이드 값으로 확률로 변경하기  
from scipy.special import expit #expit 시그모이드 함수 
print(expit(decisions)) 
`````



##### 로지스틱 회귀로 다중 분류 수행하기

`````python
#학습하기 
lr = LogisticRegression(C=20, max_iter=1000) #C - 학습 규제 강도 (큰 숫자 Overfitting 방지) 기본:1 , max_iter 반복의 횟수 기본 : 100
lr.fit(train_scaled, train_target)

#테스트 데이터 분류하기 
print(lr.score(train_scaled, train_target))
print(lr.predict(test_scaled[:5]))

#확률 출력 
proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=3))

# 분류 클래스 
print(lr.classes_)

#계수와 절편의 형태 - 7행, 5열 / 7행: z값 7개-분류별로 z값을 구하였음, 5열: 특성 5개 
print(lr.coef_.shape, lr.intercept_.shape)

# z 값
decision = lr.decision_function(test_scaled[:5]) #z1~z7개의 값
print(np.round(decision, decimals=2)) #z1~z7개의 값 출력 

# 시그모이드 값 
from scipy.special import softmax #(다중 분류에서는 softmax 함수 사용) #softmax는 여러 개의 선형 방정식의 출력값을 0~1 사이로 압축하고 전체 합이 1이 되도록 만드는 함수 
proba = softmax(decision, axis=1)
print(np.round(proba, decimals=3))
`````

